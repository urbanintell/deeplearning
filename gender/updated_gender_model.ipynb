{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "import keras\n",
    "import os, shutil\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras .callbacks import TensorBoard\n",
    "from tensorflow.keras  import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from PIL import ImageFile\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_women_dir = './gender/Female'\n",
    "base_dir = './gender/categories'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "\n",
    "train_women_dir = os.path.join(train_dir, 'women')\n",
    "validation_women_dir = os.path.join(validation_dir, 'women')\n",
    "test_women_dir = os.path.join(test_dir, 'women')\n",
    "\n",
    "\n",
    "train_men_dir = os.path.join(train_dir, 'men')\n",
    "validation_men_dir = os.path.join(validation_dir, 'men')\n",
    "test_men_dir = os.path.join(test_dir, 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training male images: 700\n",
      "total training female images: 679\n",
      "total validation male images: 50\n",
      "total validation female images: 57\n",
      "total test male images: 246\n",
      "total test women images: 251\n"
     ]
    }
   ],
   "source": [
    "print('total training male images:', len(os.listdir(train_men_dir)))\n",
    "print('total training female images:', len(os.listdir(train_women_dir)))\n",
    "print('total validation male images:', len(os.listdir(validation_men_dir)))\n",
    "print('total validation female images:', len(os.listdir(validation_women_dir)))\n",
    "print('total test male images:', len(os.listdir(test_men_dir)))\n",
    "print('total test women images:', len(os.listdir(test_women_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_25 (Conv2D)           (None, 148, 223, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 74, 111, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 72, 109, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 36, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 34, 52, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 17, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 28288)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 256)               7241984   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 7,303,649\n",
      "Trainable params: 7,303,649\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# parameters should not exceed number of datapoints in training per label by a factor of 10.\n",
    "# small dataste NN was able to memorize was not able to generalize\n",
    "# model can capture variations\n",
    "# smaller means number of parameters \n",
    "# increasing layers m decrease parameters\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',\n",
    "                        input_shape=(150, 225, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "# sgd = optimizers.SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rmsprop = optimizers.RMSprop(lr=1e-3, decay=1e-6)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=rmsprop,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1377 images belonging to 2 classes.\n",
      "Found 105 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,                    \n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# decreased batch size from 10 to 5\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x225\n",
    "        target_size=(150, 225),\n",
    "        batch_size=50,\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 225),\n",
    "        batch_size=5,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " 99/100 [============================>.] - ETA: 4s - loss: 0.1539 - acc: 0.9458\n",
      "Epoch 00001: val_acc improved from -inf to 0.57200, saving model to checkpoint.01.hdf5\n",
      "100/100 [==============================] - 496s 5s/step - loss: 0.1535 - acc: 0.9459 - val_loss: 1.6493 - val_acc: 0.5720\n",
      "Epoch 2/30\n",
      " 64/100 [==================>...........] - ETA: 2:05 - loss: 0.1335 - acc: 0.9510"
     ]
    }
   ],
   "source": [
    "# val_loss_early_stop = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "#                               min_delta=0.1,\n",
    "#                               patience=3,\n",
    "#                               verbose=0, mode='auto')\n",
    "\n",
    "# val_acc_early_stop = keras.callbacks.EarlyStopping(monitor='val_acc',\n",
    "#                               min_delta=0.1,\n",
    "#                               patience=3,\n",
    "#                               verbose=0, mode='auto')\n",
    "\n",
    "CHECKPOINT_FILE_PATH = 'checkpoint.{epoch:02d}.hdf5'\n",
    "\n",
    "checkpoint = ModelCheckpoint(CHECKPOINT_FILE_PATH,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=False,\n",
    "                             mode='auto',\n",
    "                             period=1)\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Improvement after changing optimizer: https://keras.io/optimizers/\n",
    "# Added SGD optimizer\n",
    "# https://stackoverflow.com/questions/37213388/keras-accuracy-does-not-change\n",
    "\n",
    "# board = TensorBoard(log_dir='./output',\n",
    "#                             histogram_freq=0,\n",
    "#                             batch_size=16,\n",
    "#                             write_graph=True,\n",
    "#                             write_grads=False,\n",
    "#                             write_images=False,\n",
    "#                             embeddings_layer_names=None,\n",
    "#                             embeddings_metadata=None,\n",
    "#                             embeddings_data=None)\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=100,\n",
    "      shuffle=True,\n",
    "      callbacks=[checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "model.save('thesatorialist.h5')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
